{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Because the results show different cubic-micrometer values, I wil try to normalize the data, which puts every voluem value in relation to the highest volume value. This will create results from 0 to 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.statology.org/normalize-data-between-0-and-1/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize volumes from manual thresholding method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_excel(\"Volume_Calculation_manualThresh.xlsx\") # uncomment code in cell above if pandas can't read the file \n",
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = db[\"volume\"].min()\n",
    "max_value = db[\"volume\"].max()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the values using the formula \n",
    "### zi = (xi – min(x)) / (max(x) – min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db['normalized'] = (db['volume'] - min_value) / (max_value - min_value)\n",
    "db.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize volumes from SHAPR thresholding method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = pd.read_excel(\"Volume_Calculation_bySHAPR.xlsx\") \n",
    "min_value1 = db1[\"volume\"].min()\n",
    "max_value1 = db1[\"volume\"].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1['normalized'] = (db1['volume'] - min_value1) / (max_value1 - min_value1)\n",
    "db1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first plot\n",
    "cv = db[\"normalized\"]\n",
    "cv = cv.to_numpy()\n",
    "mean, std = norm.fit(cv)\n",
    "\n",
    "plt.hist(cv, bins=25, density=True, color='#0504aa', alpha=0.7, rwidth=0.85)\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mean, std)\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "title = \"Manual Threhsholding. Results XXX: mean = %.2f,  std = %.2f\" % (mean, std)\n",
    "plt.title(title)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#second plot\n",
    "cv1 = db1[\"normalized\"]\n",
    "cv1 = cv1.to_numpy()\n",
    "mean1, std1 = norm.fit(cv1)\n",
    "\n",
    "plt.hist(cv1, bins=25, density=True, color='#0504aa', alpha=0.7, rwidth=0.85)\n",
    "xmin1, xmax1 = plt.xlim()\n",
    "x1 = np.linspace(xmin1, xmax1, 100)\n",
    "p1 = norm.pdf(x1, mean1, std1)\n",
    "plt.plot(x1, p1, 'k', linewidth=2)\n",
    "title1 = \"Thresholding by SHAPR. Results XXX: mean = %.2f,  std = %.2f\" % (mean1, std1)\n",
    "plt.title(title1)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I want to know how the results of my prediction differ from the results of the training set used by the SHAPR authors\n",
    "the datasets can also be find here: https://zenodo.org/record/7031924#.ZFPXCC9n5pQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = pd.read_csv(\"SHAPR_features.csv\")\n",
    "db2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value2 = db2[\"volume\"].min()\n",
    "max_value2 = db2[\"volume\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2['normalized'] = (db2['volume'] - min_value2) / (max_value2 - min_value2)\n",
    "db2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#first plot\n",
    "cv = db[\"normalized\"]\n",
    "cv = cv.to_numpy()\n",
    "mean, std = norm.fit(cv)\n",
    "\n",
    "plt.hist(cv, bins=25, density=True, color='#0504aa', alpha=0.7, rwidth=0.85)\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mean, std)\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "title = \"Manual Threhsholding. Results XXX: mean = %.2f,  std = %.2f\" % (mean, std)\n",
    "plt.title(title)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#second plot\n",
    "cv1 = db1[\"normalized\"]\n",
    "cv1 = cv1.to_numpy()\n",
    "mean1, std1 = norm.fit(cv1)\n",
    "\n",
    "plt.hist(cv1, bins=25, density=True, color='#0504aa', alpha=0.7, rwidth=0.85)\n",
    "xmin1, xmax1 = plt.xlim()\n",
    "x1 = np.linspace(xmin1, xmax1, 100)\n",
    "p1 = norm.pdf(x1, mean1, std1)\n",
    "plt.plot(x1, p1, 'k', linewidth=2)\n",
    "title1 = \"Thresholding by SHAPR. Results XXX: mean = %.2f,  std = %.2f\" % (mean1, std1)\n",
    "plt.title(title1)\n",
    "plt.show()\n",
    "\n",
    "#third plot\n",
    "cv2 = db2[\"normalized\"]\n",
    "cv2 = cv2.to_numpy()\n",
    "mean2, std2 = norm.fit(cv2)\n",
    "\n",
    "\n",
    "plt.hist(cv2, bins=25, density=True, color='#0504aa', alpha=0.7, rwidth=0.85)\n",
    "xmin2, xmax2 = plt.xlim()\n",
    "x2 = np.linspace(xmin2, xmax2, 100)\n",
    "p2 = norm.pdf(x2, mean2, std2)\n",
    "plt.plot(x2, p2, 'k', linewidth=2)\n",
    "title2 = \"SHAPR trainingset. Results XXX: mean = %.2f,  std = %.2f\" % (mean2, std2)\n",
    "plt.title(title2)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now i want to see how the normalized predictions differ with the absorption analysis data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db3 = pd.read_excel(\"Absorption_Analysis_Cleaned.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value3 = db3[\"CV\"].min()\n",
    "max_value3 = db3[\"CV\"].max()\n",
    "db3['normalized'] = (db3['CV'] - min_value3) / (max_value3 - min_value3)\n",
    "db3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first plot\n",
    "cv = db[\"normalized\"]\n",
    "cv = cv.to_numpy()\n",
    "mean, std = norm.fit(cv)\n",
    "\n",
    "plt.hist(cv, bins=25, density=True, color='#0504aa', alpha=0.7, rwidth=0.85)\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mean, std)\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "title = \"Manual Thresholding. Results XXX: mean = %.2f,  std = %.2f\" % (mean, std)\n",
    "plt.title(title)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#second plot\n",
    "cv1 = db1[\"normalized\"]\n",
    "cv1 = cv1.to_numpy()\n",
    "mean1, std1 = norm.fit(cv1)\n",
    "\n",
    "plt.hist(cv1, bins=25, density=True, color='#0504aa', alpha=0.7, rwidth=0.85)\n",
    "xmin1, xmax1 = plt.xlim()\n",
    "x1 = np.linspace(xmin1, xmax1, 100)\n",
    "p1 = norm.pdf(x1, mean1, std1)\n",
    "plt.plot(x1, p1, 'k', linewidth=2)\n",
    "title1 = \"Thresholding by SHAPR. Results XXX: mean = %.2f,  std = %.2f\" % (mean1, std1)\n",
    "plt.title(title1)\n",
    "plt.show()\n",
    "\n",
    "#third plot\n",
    "cv2 = db2[\"normalized\"]\n",
    "cv2 = cv2.to_numpy()\n",
    "mean2, std2 = norm.fit(cv2)\n",
    "\n",
    "\n",
    "plt.hist(cv2, bins=25, density=True, color='#0504aa', alpha=0.7, rwidth=0.85)\n",
    "xmin2, xmax2 = plt.xlim()\n",
    "x2 = np.linspace(xmin2, xmax2, 100)\n",
    "p2 = norm.pdf(x2, mean2, std2)\n",
    "plt.plot(x2, p2, 'k', linewidth=2)\n",
    "title2 = \"SHAPR trainingset. Results XXX: mean = %.2f,  std = %.2f\" % (mean2, std2)\n",
    "plt.title(title2)\n",
    "plt.show()\n",
    "\n",
    "#fourth plot\n",
    "cv3 = db3[\"normalized\"]\n",
    "cv3 = cv3.to_numpy()\n",
    "mean3, std3 = norm.fit(cv3)\n",
    "\n",
    "\n",
    "plt.hist(cv3, bins=25, density=True, color='#0504aa', alpha=0.7, rwidth=0.85)\n",
    "xmin3, xmax3 = plt.xlim()\n",
    "x3 = np.linspace(xmin3, xmax3, 100)\n",
    "p3 = norm.pdf(x3, mean3, std3)\n",
    "plt.plot(x3, p3, 'k', linewidth=2)\n",
    "title3 = \"Absorption Analysis. Results XXX: mean = %.2f,  std = %.2f\" % (mean3, std3)\n",
    "plt.title(title3)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create overlayd histograms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictions and absorption analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "cv = db[\"normalized\"]\n",
    "cv = cv.to_numpy()\n",
    "mean, std = norm.fit(cv)\n",
    "\n",
    "plt.hist(cv, bins=25, density=True, color='blue', alpha=0.7, rwidth=0.85)\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mean, std)\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "\n",
    "cv3 = db3[\"normalized\"]\n",
    "cv3 = cv3.to_numpy()\n",
    "mean3, std3 = norm.fit(cv3)\n",
    "\n",
    "plt.hist(cv3, bins=25, density=True, color='pink', alpha=0.7, rwidth=0.85)\n",
    "xmin3, xmax3 = plt.xlim()\n",
    "x3 = np.linspace(xmin3, xmax3, 100)\n",
    "p3 = norm.pdf(x3, mean3, std3)\n",
    "plt.plot(x3, p3, 'm', linewidth=2)\n",
    "title = \"Blue: Predictions, Pink: Absorption Analysis\"\n",
    "plt.title(title)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = db[\"normalized\"]\n",
    "cv = cv.to_numpy()\n",
    "mean, std = norm.fit(cv)\n",
    "\n",
    "plt.hist(cv, bins=25, density=True, color='blue', alpha=0.7, rwidth=0.85)\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mean, std)\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "\n",
    "cv2 = db2[\"normalized\"]\n",
    "cv2 = cv2.to_numpy()\n",
    "mean2, std2 = norm.fit(cv2)\n",
    "\n",
    "\n",
    "plt.hist(cv2, bins=25, density=True, color='pink', alpha=0.7, rwidth=0.85)\n",
    "xmin2, xmax2 = plt.xlim()\n",
    "x2 = np.linspace(xmin2, xmax2, 100)\n",
    "p2 = norm.pdf(x2, mean2, std2)\n",
    "plt.plot(x2, p2, 'm', linewidth=2)\n",
    "title = \"Blue: Predictions, Pink: SHAPR trainingset\"\n",
    "plt.title(title)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
